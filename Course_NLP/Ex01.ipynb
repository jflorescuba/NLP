{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uso de tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-08 12:29:05.753319: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-04-08 12:29:05.968247: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-04-08 12:29:05.968292: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-04-08 12:29:06.002182: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-04-08 12:29:06.080954: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-08 12:29:06.838526: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'love': 1, 'my': 2, 'i': 3, 'dog': 4, 'cat': 5, 'you': 6}\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "#Define input sentences\n",
    "sentences = [\n",
    "            'I love my dog',\n",
    "            'I love my cat',\n",
    "            'You love my dog'\n",
    "]\n",
    "\n",
    "#Initialize the Tokenizer class\n",
    "tokenizer = Tokenizer(num_words = 100)\n",
    "\n",
    "#Generate indices for each word in the corpus\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "\n",
    "#Get the indices and print it\n",
    "word_index = tokenizer.word_index\n",
    "print(word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_index['cat']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text to Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'my': 1, 'love': 2, 'dog': 3, 'i': 4, 'you': 5, 'cat': 6, 'do': 7, 'think': 8, 'is': 9, 'amazing': 10}\n",
      "[[4, 2, 1, 3], [4, 2, 1, 6], [5, 2, 1, 3], [7, 5, 8, 1, 3, 9, 10]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "#Define input sentences\n",
    "sentences = [\n",
    "            'I love my dog',\n",
    "            'I love my cat',\n",
    "            'You love my dog',\n",
    "            'Do you think my dog is amazing?'\n",
    "]\n",
    "\n",
    "#Initialize the Tokenizer class\n",
    "tokenizer = Tokenizer(num_words = 100)\n",
    "\n",
    "#Generate indices for each word in the corpus\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "\n",
    "#Get the indices\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "#Generate list of token sequences\n",
    "sequences = tokenizer.texts_to_sequences(sentences)\n",
    "\n",
    "print(word_index)\n",
    "print(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4, 2, 1, 3], [1, 3, 1]]\n"
     ]
    }
   ],
   "source": [
    "test_data = [\n",
    "            'i really love my dog',\n",
    "            'my dog loves my manatee'\n",
    "]\n",
    "\n",
    "test_seq = tokenizer.texts_to_sequences(test_data)\n",
    "print(test_seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Out-of-vocabulary tokens. Consider words that are not found in the word_index dictionary\n",
    "#### oov : out-of-vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5, 1, 3, 2, 4], [2, 4, 1, 2, 1]]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "#Define input sentences\n",
    "sentences = [\n",
    "            'I love my dog',\n",
    "            'I love my cat',\n",
    "            'You love my dog',\n",
    "            'Do you think my dog is amazing?'\n",
    "]\n",
    "\n",
    "#Initialize the Tokenizer class\n",
    "tokenizer = Tokenizer(num_words = 100, oov_token = \"<00V>\")\n",
    "\n",
    "#Generate indices for each word in the corpus\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "\n",
    "#Get the indices\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "#Generate list of token sequences\n",
    "sequences = tokenizer.texts_to_sequences(sentences)\n",
    "\n",
    "test_data = [\n",
    "            'i really love my dog',\n",
    "            'my dog loves my manatee'\n",
    "]\n",
    "\n",
    "test_seq = tokenizer.texts_to_sequences(test_data)\n",
    "print(test_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<00V>': 1,\n",
       " 'my': 2,\n",
       " 'love': 3,\n",
       " 'dog': 4,\n",
       " 'i': 5,\n",
       " 'you': 6,\n",
       " 'cat': 7,\n",
       " 'do': 8,\n",
       " 'think': 9,\n",
       " 'is': 10,\n",
       " 'amazing': 11}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Padding \n",
    "### para uniformizar la longitud de la data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Word Index = {'<00V>': 1, 'my': 2, 'love': 3, 'dog': 4, 'i': 5, 'you': 6, 'cat': 7, 'do': 8, 'think': 9, 'is': 10, 'amazing': 11}\n",
      "\n",
      " Sequences = [[5, 3, 2, 4], [5, 3, 2, 7], [6, 3, 2, 4], [8, 6, 9, 2, 4, 10, 11]]\n",
      "\n",
      " Padded Sequences :\n",
      "[[ 5  3  2  4  0]\n",
      " [ 5  3  2  7  0]\n",
      " [ 6  3  2  4  0]\n",
      " [ 9  2  4 10 11]]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "#Define input sentences\n",
    "sentences = [\n",
    "            'I love my dog',\n",
    "            'I love my cat',\n",
    "            'You love my dog',\n",
    "            'Do you think my dog is amazing?'\n",
    "]\n",
    "\n",
    "#Initialize the Tokenizer class\n",
    "tokenizer = Tokenizer(num_words = 100, oov_token = \"<00V>\")\n",
    "\n",
    "#Generate indices for each word in the corpus\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "\n",
    "#Get the indices\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "#Generate list of token sequences\n",
    "sequences = tokenizer.texts_to_sequences(sentences)\n",
    "\n",
    "#Pad the sequences to a uniform length\n",
    "#padded = pad_sequences(sequences)\n",
    "padded = pad_sequences(sequences, padding='post', truncating= 'pre', maxlen= 5)\n",
    "\n",
    "print(\"\\n Word Index =\", word_index)\n",
    "print(\"\\n Sequences =\",sequences)\n",
    "print(\"\\n Padded Sequences :\")\n",
    "print(padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 3, 2, 4, 0], dtype=int32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padded[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Embeddings (assigns vector to each word with its associated sentiment)\n",
    "##### IMDB reviews\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.15.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "imdb, info = tfds.load(\"imdb_reviews\", with_info=True, as_supervised= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000\n",
      "25000\n"
     ]
    }
   ],
   "source": [
    "info\n",
    "imdb\n",
    "print(len(imdb['train']))\n",
    "print(len(imdb['test']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data is split into 25000 samples for training and 25000 for testing\n",
    "\n",
    "import numpy as np\n",
    "train_data, test_data = imdb['train'], imdb['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_sentences = []\n",
    "training_labels = []\n",
    "\n",
    "testing_sentences = []\n",
    "testing_labels = []\n",
    "\n",
    "#loop over all training examples and save the sentences and labels\n",
    "for s,l in train_data:\n",
    "    training_sentences.append(s.numpy().decode('utf8'))\n",
    "    training_labels.append(l.numpy())\n",
    "\n",
    "# s extracts first element, l extracts second element\n",
    "# .numpy() converts a tensorflow tensor to a numpy array\n",
    "# .decode('utf8') converts into a regular python string \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This was an absolutely terrible movie. Don't be lured in by Christopher Walken or Michael Ironside. Both are great actors, but this must simply be their worst role in history. Even their great acting could not redeem this movie's ridiculous storyline. This movie is an early nineties US propaganda piece. The most pathetic scenes were those when the Columbian rebels were making their cases for revolutions. Maria Conchita Alonso appeared phony, and her pseudo-love affair with Walken was nothing but a pathetic emotional plug in a movie that was devoid of any real meaning. I am disappointed that there are movies like this, ruining actor's like Christopher Walken's good name. I could barely sit through it. 0\n",
      "I have been known to fall asleep during films, but this is usually due to a combination of things including, really tired, being warm and comfortable on the sette and having just eaten a lot. However on this occasion I fell asleep because the film was rubbish. The plot development was constant. Constantly slow and boring. Things seemed to happen, but with no explanation of what was causing them or why. I admit, I may have missed part of the film, but i watched the majority of it and everything just seemed to happen of its own accord without any real concern for anything else. I cant recommend this film at all. 0\n",
      "Mann photographs the Alberta Rocky Mountains in a superb fashion, and Jimmy Stewart and Walter Brennan give enjoyable performances as they always seem to do. <br /><br />But come on Hollywood - a Mountie telling the people of Dawson City, Yukon to elect themselves a marshal (yes a marshal!) and to enforce the law themselves, then gunfighters battling it out on the streets for control of the town? <br /><br />Nothing even remotely resembling that happened on the Canadian side of the border during the Klondike gold rush. Mr. Mann and company appear to have mistaken Dawson City for Deadwood, the Canadian North for the American Wild West.<br /><br />Canadian viewers be prepared for a Reefer Madness type of enjoyable howl with this ludicrous plot, or, to shake your head in disgust. 0\n",
      "This is the kind of film for a snowy Sunday afternoon when the rest of the world can go ahead with its own business as you descend into a big arm-chair and mellow for a couple of hours. Wonderful performances from Cher and Nicolas Cage (as always) gently row the plot along. There are no rapids to cross, no dangerous waters, just a warm and witty paddle through New York life at its best. A family film in every sense and one that deserves the praise it received. 1\n",
      "As others have mentioned, all the women that go nude in this film are mostly absolutely gorgeous. The plot very ably shows the hypocrisy of the female libido. When men are around they want to be pursued, but when no \"men\" are around, they become the pursuers of a 14 year old boy. And the boy becomes a man really fast (we should all be so lucky at this age!). He then gets up the courage to pursue his true love. 1\n"
     ]
    }
   ],
   "source": [
    "len(training_labels)\n",
    "for i in range(0,5):\n",
    "    print(training_sentences[i],training_labels[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loop over all test examples and save the sentences and labels\n",
    "for s,l in test_data:\n",
    "    testing_sentences.append(s.numpy().decode('utf8'))\n",
    "    testing_labels.append(l.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are films that make careers. For George Romero, it was NIGHT OF THE LIVING DEAD; for Kevin Smith, CLERKS; for Robert Rodriguez, EL MARIACHI. Add to that list Onur Tukel's absolutely amazing DING-A-LING-LESS. Flawless film-making, and as assured and as professional as any of the aforementioned movies. I haven't laughed this hard since I saw THE FULL MONTY. (And, even then, I don't think I laughed quite this hard... So to speak.) Tukel's talent is considerable: DING-A-LING-LESS is so chock full of double entendres that one would have to sit down with a copy of this script and do a line-by-line examination of it to fully appreciate the, uh, breadth and width of it. Every shot is beautifully composed (a clear sign of a sure-handed director), and the performances all around are solid (there's none of the over-the-top scenery chewing one might've expected from a film like this). DING-A-LING-LESS is a film whose time has come. 1\n",
      "A blackly comic tale of a down-trodden priest, Nazarin showcases the economy that Luis Bunuel was able to achieve in being able to tell a deeply humanist fable with a minimum of fuss. As an output from his Mexican era of film making, it was an invaluable talent to possess, with little money and extremely tight schedules. Nazarin, however, surpasses many of Bunuel's previous Mexican films in terms of the acting (Francisco Rabal is excellent), narrative and theme.<br /><br />The theme, interestingly, is something that was explored again in Viridiana, made three years later in Spain. It concerns the individual's struggle for humanity and altruism amongst a society that rejects any notion of virtue. Father Nazarin, however, is portrayed more sympathetically than Sister Viridiana. Whereas the latter seems to choose charity because she wishes to atone for her (perceived) sins, Nazarin's whole existence and reason for being seems to be to help others, whether they (or we) like it or not. The film's last scenes, in which he casts doubt on his behaviour and, in a split second, has to choose between the life he has been leading or the conventional life that is expected of a priest, are so emotional because they concern his moral integrity and we are never quite sure whether it remains intact or not.<br /><br />This is a remarkable film and I would urge anyone interested in classic cinema to seek it out. It is one of Bunuel's most moving films, and encapsulates many of his obsessions: frustrated desire, mad love, religious hypocrisy etc. In my view 'Nazarin' is second only to 'The Exterminating Angel', in terms of his Mexican movies, and is certainly near the top of the list of Bunuel's total filmic output. 1\n",
      "Scary Movie 1-4, Epic Movie, Date Movie, Meet the Spartans, Not another Teen Movie and Another Gay Movie. Making \"Superhero Movie\" the eleventh in a series that single handily ruined the parody genre. Now I'll admit it I have a soft spot for classics such as Airplane and The Naked Gun but you know you've milked a franchise so bad when you can see the gags a mile off. In fact the only thing that might really temp you into going to see this disaster is the incredibly funny but massive sell-out Leslie Neilson.<br /><br />You can tell he needs the money, wither that or he intends to go down with the ship like a good Capitan would. In no way is he bringing down this genre but hell he's not helping it. But if I feel sorry for anybody in this film its decent actor Drake Bell who is put through an immense amount of embarrassment. The people who are put through the largest amount of torture by far however is the audience forced to sit through 90 minutes of laughless bile no funnier than herpes.<br /><br />After spoofing disaster films in Airplane!, police shows in The Naked Gun, and Hollywood horrors in Scary Movie 3 and 4, producer David Zucker sets his satirical sights on the superhero genre with this anarchic comedy lampooning everything from Spider-Man to X-Men and Superman Returns.<br /><br />Shortly after being bitten by a genetically altered dragonfly, high-school outcast Rick Riker (Drake Bell) begins to experience a startling transformation. Now Rick's skin is as strong as steel, and he possesses the strength of ten men. Determined to use his newfound powers to fight crime, Rick creates a special costume and assumes the identity of The Dragonfly -- a fearless crime fighter dedicated to keeping the streets safe for law-abiding citizens.<br /><br />But every superhero needs a nemesis, and after Lou Landers (Christopher McDonald) is caught in the middle of an experiment gone horribly awry, he develops the power to leech the life force out of anyone he meets and becomes the villainous Hourglass. Intent on achieving immortality, the Hourglass attempts to gather as much life force as possible as the noble Dragonfly sets out to take down his archenemy and realize his destiny as a true hero. Craig Mazin writes and directs this low-flying spoof.<br /><br />featuring Tracy Morgan, Pamela Anderson, Leslie Nielsen, Marion Ross, Jeffrey Tambor, and Regina Hall.<br /><br />Hell Superhero Movie may earn some merit in the fact that it's a hell of a lot better than Meet the Spartans and Epic Movie. But with great responsibility comes one of the worst outings of 2008 to date. Laughless but a little less irritating than Meet the Spartans. And in the same sense much more forgettable than meet the Spartans. But maybe that's a good reason. There are still some of us trying to scrape away the stain that was Meet the Spartans from our memory.<br /><br />My final verdict? Avoid, unless you're one of thoses people who enjoy such car crash cinema. As bad as Date Movie and Scary Movie 2 but not quite as bad as Meet the Spartans or Epic Movie. Super Villain. 0\n",
      "Poor Shirley MacLaine tries hard to lend some gravitas to this mawkish, gag-inducing \"feel-good\" movie, but she's trampled by the run-away sentimentality of a film that's not the least bit grounded in reality.<br /><br />This was directed by Curtis Hanson? Did he have a lobotomy since we last heard from him? Hanson can do effective drama sprinkled with comedy, as evidenced by \"Wonder Boys.\" So I don't know what happened to him here. This is the kind of movie that doesn't want to accept that life is messy and fussy, and that neat, tidy endings (however implausible they might be) might make for a nice closing shot, but come across as utterly phony if the people watching the film have been through anything remotely like what the characters in the film go through.<br /><br />My wife and I made a game of calling out the plot points before they occurred -- e.g. \"the old man's going to teach her to read and then drop dead.\" Bingo! This is one of those movies where the characters give little speeches summarizing their emotional problems, making you wonder why they still have emotional problems if they're that aware of what's causing them. Toni Collette (a fine actress, by the way, and one of my favorites if not given a lot to work with here), gives a speech early on about why she buys so many shoes and never wears them, spelling out in flashing neon the film's awkward connecting motif. At that moment, I knew what I was in for, and the film was a downward spiral from there.<br /><br />Grade: C- 0\n",
      "As a former Erasmus student I enjoyed this film very much. It was so realistic and funny. It really picked up the spirit that exists among Erasmus students. I hope, many other students will follow this experience, too. However, I wonder if this movie is all that interesting to watch for people with no international experience. But at least one of my friends who has never gone on Erasmus also enjoyed it very much. I give it 9 out of 10. 1\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,5):\n",
    "    print(testing_sentences[i],testing_labels[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "training_labels_final = np.array(training_labels)\n",
    "testing_labels_final = np.array(testing_labels)\n",
    "print(type(testing_labels))\n",
    "print(type(testing_labels_final))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 10000\n",
    "embedding_dim = 16\n",
    "max_length = 120\n",
    "trunc_type = 'post'\n",
    "oov_tok = \"<OOV>\"\n",
    "\n",
    "#creating tokenizer object\n",
    "tokenizer = Tokenizer(num_words = vocab_size, oov_token=oov_tok)\n",
    "# training the tokenizer on the training_sentences list\n",
    "tokenizer.fit_on_texts(training_sentences)\n",
    "#storing the mapping between each word and its corresponding integer index\n",
    "word_index = tokenizer.word_index\n",
    "#Converting each sentence into a sequence of integers\n",
    "sequences = tokenizer.texts_to_sequences(training_sentences)\n",
    "#padding or truncating the sequences to ensure they all have the same length\n",
    "padded = pad_sequences(sequences, maxlen=max_length, truncating=trunc_type)\n",
    "\n",
    "#same opperaton as the last two ones\n",
    "testing_sequences = tokenizer.texts_to_sequences(testing_sentences)\n",
    "testing_padded = pad_sequences(testing_sequences, maxlen=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    #Embedding layer : from integer sequences to dense vector representation (embeddings)\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
    "    #The embedding layer is a 3D tensor (batch_size, sequence_length, embedding_dim)\n",
    "    #Flatten layer reshapes this tensor into a single dimension for each sample in the batch\n",
    "    tf.keras.layers.Flatten(),\n",
    "    # or tf.keras.layers.GlobalAveragePooling1D(),\n",
    "    #Fully-connected layer with 6 neurons\n",
    "    tf.keras.layers.Dense(6, activation='relu'),\n",
    "    #Fully-connected layer with 1 neuron\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 120, 16)           160000    \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 1920)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 6)                 11526     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 7         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 171533 (670.05 KB)\n",
      "Trainable params: 171533 (670.05 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='adam',metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "782/782 [==============================] - 3s 3ms/step - loss: 0.4925 - accuracy: 0.7397 - val_loss: 0.3819 - val_accuracy: 0.8285\n",
      "Epoch 2/10\n",
      "782/782 [==============================] - 2s 2ms/step - loss: 0.2354 - accuracy: 0.9090 - val_loss: 0.3727 - val_accuracy: 0.8349\n",
      "Epoch 3/10\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0865 - accuracy: 0.9775 - val_loss: 0.4465 - val_accuracy: 0.8269\n",
      "Epoch 4/10\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0199 - accuracy: 0.9975 - val_loss: 0.5276 - val_accuracy: 0.8262\n",
      "Epoch 5/10\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0048 - accuracy: 0.9998 - val_loss: 0.5837 - val_accuracy: 0.8285\n",
      "Epoch 6/10\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.6332 - val_accuracy: 0.8288\n",
      "Epoch 7/10\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 8.2425e-04 - accuracy: 1.0000 - val_loss: 0.6747 - val_accuracy: 0.8298\n",
      "Epoch 8/10\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 4.5517e-04 - accuracy: 1.0000 - val_loss: 0.7160 - val_accuracy: 0.8297\n",
      "Epoch 9/10\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 2.6954e-04 - accuracy: 1.0000 - val_loss: 0.7568 - val_accuracy: 0.8283\n",
      "Epoch 10/10\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 1.5685e-04 - accuracy: 1.0000 - val_loss: 0.7888 - val_accuracy: 0.8294\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x734884530a00>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "model.fit(padded,\n",
    "          training_labels_final,\n",
    "          epochs=num_epochs,\n",
    "          validation_data=(testing_padded, testing_labels_final))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
